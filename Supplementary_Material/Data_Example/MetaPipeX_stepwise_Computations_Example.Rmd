---
title: "MetaPipeX_stepwise_Computations_Example"
author: "Jens Fuenderich"
date: "2022-12-04"
output: pdf_document
---

## Data Example: stepwise computations

This example demonstrates how the *MetaPipeX* framework may be used to analyze and harmonize *multi-lab* data. In this document we demonstrate the functions for each individual step of the pipeline. The *MetaPipeX::full_pipeline* function implements all of the standardized functions and provides the full pipeline starting at the level of individual participant data (*IPD*). For a demonstration of the *MetaPipeX::full_pipeline* function, refer to the MetaPipeX_full_pipeline_Example.Rmd [on github](https://github.com/JensFuenderich/MetaPipeX/tree/main/Supplementary_Material/Data_Example).

![Figure 1: Graphical Representation of the Example](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/MetaPipeX_Tutorial.png)

The example showcases the three components of the pipeline, using data by Dang et al. (2021) and by Hagger et al. (*RRR4*, 2016). This example is structured in four steps that should be executed in the order they are presented:

1.  Mapping the example to the pipeline
2.  Analysis & Documentation in R
3.  Analysis & Documentation in the Shiny App
4.  Combining data sets in the Shiny App

## 1. Mapping the example to the pipeline

The MetaPipeX pipeline consists of unstandardized and standardized computational steps. All computations performed on IPD and higher levels of aggregation are standardized by the pipeline and need no further adaption. The steps preceding *IPD* should be documented for each meta-analytical-study-collection (*MASC*) individually. For our example we have provided short keywords summarizing important aspects these computational steps in Fig. 1. Each *data collection site* (k = 23) extracted the dependent variable (*DV*) from the raw data, an indication for the Group and provided columns that specify exclusions of participants (or, more broadly, rows) according to the data cleaning protocol. We could not find the raw data in the *RRR4* OSF (and thus can not recreate the Item Level data from raw data as depicted in the MetaPipeX pipeline [on github](https://github.com/JensFuenderich/MetaPipeX/blob/main/Supplementary_Material/Graphics/MetaPipeX_Pipeline.pdf)). But they provide R-code that documents the computations (<https://osf.io/nb8uj>) and decided that they would represent step 1 in the pipeline. For a more detailed verbal description of the variables in the data, please refer to the *RRR4* publication. We will use the aggregated form of the results, the *IPD* file (merged across sites) that is provided in the *RRR4* repository (<https://osf.io/t3mns>) as the basis for further computations. Step 2, creating the standardized *IPD* format, is documented in an R-markdown file on GitHub. To bring the data into the *IPD* format, we need to select the relevant *DV*, which is often aggregated from multiple items, the independent variable, usually a treatment or control group indication, and apply exclusions. We can use the experimental manipulation, in which participants completed either a depletion or no depletion version of the letter "e" task (Baumeister et al., 1998, Study 4) as the Group variable and the reaction time variance (*RTV*) from a multi-source interference task (Bush et al., 2003) as the *DV*.

![Figure 2: The adapted MetaPipeX pipeline.](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/MetaPipeX_Example_Hagger.png)

## 2. Analysis & Documentation in R

In this step, we apply the structure depicted in Fig. 4 as a researcher in a multi-lab project could. In the case of the example that means we use the *RRR4* data set RRR-MergedSubjectData.csv, run all computational steps and document the resulting data sets (exporting them alongside with codebooks). In order to execute the code, we need to install and load the necessary packages and create the output folder. By default, getwd(), the current working directory in R, is assigned to the path variable. We could define another output path by replacing getwd() with a path of choice.

#### Install & Load Packages, Create Output Folder

```{r setup}

## select packages 
packages <- c("magrittr", 
              "osfr", 
              "readr",
              "renv",
              "stats")

## check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x)                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})

## install MetaPipeX from github using renv 
renv::install("JensFuenderich/MetaPipeX/R-Package")

## set up output folder 
# define output path (set to working directory) 
custom_output_path <- getwd() 
# define path for output folder
output_folder_path <- paste(custom_output_path, "MetaPipeX_Output", sep = "/")
# create folder
dir.create(output_folder_path)

```

#### Documenting Raw Data

This step was done by the participating data collection sites and not documented in a way that we could recreate it. For more details, refer to the section *1. Mapping the example to the pipeline*.

Creating an empty folder lvl0.1_raw_data.

```{r documenting raw data}

## create empty folder (just to complete the file structure)
# define path to folder
folder_path_item <- paste(output_folder_path, "lvl0.1_raw_data", sep = "/")
# create folder
dir.create(folder_path_item)

```

#### Computational Step 1

This step was done by the participating data collection sites and not documented in a way that we could recreate it. For more details, refer to the section *1. Mapping the example to the pipeline*.

![Figure 3](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_1.png)

#### Documenting Item Data

Creating a folder lvl0.2_item_data and storing data in that folder.

```{r documenting item data}

## download data from osf (to output_folder_path)
# define path to folder
folder_path_item <- paste(output_folder_path, "lvl0.2_item_data", sep = "/")
# create folder
dir.create(folder_path_item)
# download 
RRR4_data_path <- osfr::osf_retrieve_file("t3mns") %>%
  osfr::osf_download(., path = folder_path_item)
Dang_data_path <- osfr::osf_retrieve_file("8egc5") %>%
  osfr::osf_download(., path = folder_path_item)

## import data from folder
Item_Level_RRR4 <- readr::read_csv(RRR4_data_path$local_path)

```

#### Computational Step 2: Creating Individual Participant Data

As depicted in Fig. 4, in the second computational step we select the relevant colums, recode the group indication and clean the data (apply exclusion criteria). Relevant columns in *Item_Level_RRR4* are *Site* (for the names of the *data collection sites*), *ExGauss.I.RTVar.MSIT* (the single numeric representation of the *DV*), *Task* (to sepperate the participants into two *groups*) and *Exclusions*. Additionally, we create columns for the multi-lab and the *MASC* (in order to differentiate this data set from others when it is harmonized).

![Figure 4](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_2.png)

Selecting relevant columns & recoding group indication. Applying data cleaning.

```{r computational step 2}

## select relevant columns 
IPD_RRR4 <- data.frame(MultiLab =  rep("RRR4", 
                                       times = length(Item_Level_RRR4$Site)), 
                       MASC = rep("Sripada", 
                                  times = length(Item_Level_RRR4$Site)), 
                       Data_Collection_Site = Item_Level_RRR4$Site , 
                       DV = Item_Level_RRR4$ExGauss.I.RTVar.MSIT, 
                       Group = base::ifelse(Item_Level_RRR4$Task == "H", 
                                            yes = 1, 
                                            no = base::ifelse(
                                              Item_Level_RRR4$Task == "E", 
                                              yes = 0, 
                                              no = NA)),
                       Exclusion = Item_Level_RRR4$Exclusions)

## apply data cleaning 
IPD_RRR4 <- base::subset(IPD_RRR4, IPD_RRR4$Exclusion == 1)

```

#### Documenting Individual Participant Data

Creating a folder lvl1_individual_participant_data and storing data in that folder.

```{r documenting ipd}

## document data & download codebook 
# define path to folder
folder_path_ipd <- paste(output_folder_path, "lvl1_individual_participant_data", sep = "/")
# create folder
dir.create(folder_path_ipd)
# export data 
readr::write_csv(IPD_RRR4, paste(folder_path_ipd, "RRR4.csv", sep = "/"))
# download codebook 
readr::write_csv(readr::read_csv(url("https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Table_Templates/lvl1_individual_participant_data/codebook_for_individual_participant_data.csv")), 
                 paste(folder_path_ipd, "codebook_for_individual_participant_data.csv", sep = "/"))

```

#### Computational Step 3: Creating Site-Level Data (Site Summaries)

Computation and documentation are carried out mostly by the function.

![Figure 5](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_3.png)

#### Documenting Site-Level Data (Site Summaries)

The only additional step we take here, is to create a folder so that the output directory is the same as the *MetaPipeX::full_pipeline* function produces.

Creating a folder lvl2_site_summaries and storing data in that folder.

```{r computational step 3 & documenting site summaries}

# define path to folder
folder_path_site_sum <- paste(output_folder_path, 
                             "lvl2_site_summaries/", 
                             sep = "/")
# create folder
dir.create(folder_path_site_sum)

# apply MetaPipeX function
Site_Summaries_RRR4 <- MetaPipeX::summarize_sites(
  data = IPD_RRR4, 
  output_folder = folder_path_site_sum)

```

#### Computational Step 4: Merging Site-Level Data (Site Summaries)

Computation and documentation are carried out mostly by the function.

![Figure 6](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_4.png)

#### Documenting Merged Site-Level Data (Site Summaries)

The only additional step we take here, is to create a folder so that the output directory is the same as the *MetaPipeX::full_pipeline* function produces.

Creating a folder lvl3_merged_site_summaries and storing data in that folder.

```{r step 4 & documenting merged site summaries}

# define path to folder
folder_path_m_site_sum <- paste(output_folder_path, 
                               "lvl3_merged_site_summaries/", 
                               sep = "/")
# create folder
dir.create(folder_path_m_site_sum)

## combine into single data set
Merged_Site_Summaries_RRR4 <- MetaPipeX::merge_site_summaries(
  data = Site_Summaries_RRR4$Site_Summaries,
  output_folder = folder_path_m_site_sum)

```

#### Computational Step 5: meta analyze MASCs

Computation and documentation are carried out mostly by the function.

![Figure 7](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_5.png)

#### Documenting MASC-Level Data (Meta Analyses)

The only additional step we take here, is to create a folder so that the output directory is the same as the *MetaPipeX::full_pipeline* function produces.

Creating a folder lvl4_meta_analyses and storing data in that folder.

```{r step 5 & documenting MASC-level data}

# define path to folder
folder_path_meta_analyses <- paste(output_folder_path, 
                                   "lvl4_meta_analyses/", 
                                   sep = "/")
# create folder
dir.create(folder_path_meta_analyses)

## run meta-analyses
Meta_Analyses_RRR4 <- MetaPipeX::meta_analyze_MASCs(
  data = Merged_Site_Summaries_RRR4$Merged_Site_Summaries, 
  output_folder = folder_path_meta_analyses)

```

#### Computational Step 6: create MetaPipeX format

Computation and documentation are carried out mostly by the function.

![Figure 8](https://raw.githubusercontent.com/JensFuenderich/MetaPipeX/main/Supplementary_Material/Graphics/Data_Example/Example_Step_6.png)

#### Documenting MetaPipeX Data

The only additional step we take here, is to create a folder so that the output directory is the same as the *MetaPipeX::full_pipeline* function produces.

Creating a folder lvl5_meta_pipe_x and storing data in that folder.

```{r step 6 & documenting MetaPipeX data}

# define path to folder
folder_path_MetaPipeX <- paste(output_folder_path, 
                                   "lvl5_meta_pipe_x/", 
                                   sep = "/")
# create folder
dir.create(folder_path_MetaPipeX)

## run meta-analyses
MetaPipeX_Data <- MetaPipeX::create_MetaPipeX_format(
  Merged_Site_Summaries = Merged_Site_Summaries_RRR4$Merged_Site_Summaries,
  Meta_Analyses = Meta_Analyses_RRR4$Meta_Analyses,
  output_folder = folder_path_MetaPipeX)

```

## 3. Analysis & Documentation in the Shiny App

Instead of using the analysis functions from the R-package, the *MetaPipeX* Shiny App allows users to quickly analyze data and download the full documentation. The app runs on a local computer from R Studio, by executing *MetaPipeX::ShinyApp()*. In this second section of the example, we first create a documentation of Dang et al. (2021). Afterwards we want to create a forest plot. As such a forest plot is already available in Dang et al. (Fig. 1), this step can help us to check whether we correctly applied the exclusion criteria (different subsets will most likely produce diverging results).

Open the Shiny App by running the next chunk and switch back to this window to read the following instructions.

#### Run App

```{r run shiny app}

MetaPipeX::ShinyApp()

```

##### Uploading and Documenting Data

The first page of the app is the Upload Data tab. Here, we choose the type of data we want to use in the app from the dropdown menu. Dang et al. (2021) provide an *IPD* set as a single .sav file. The app is able to make use of .csv, .rds and .sav files if uploaded as individual participant data, thus allowing this data file to be read. Accordingly, we select *Individual Participant Data* in the dropdown menu and import the *all labs.sav* file from 0 Item Data in the *MetaPipeX Output* folder, that we created previously, through the Browse button. Subsequently, we assign the columns that contain information relevant to the analyses. For *MultiLab* and *MASC*, we create the columns by clicking the checkbox and typing in the name of the *multi-lab* and *MASC*, in our example *Dang 2021* and *Dang 2017* (the publication from which they took their operationalization). Creating a generic column for MASC implies that all of the *data collection sites* are combined to a single meta-analysis. The .sav uses the column label *lab* as indication for the *site*, the *error rate* is the *DV* and *Condition* assigns the data points to a *Group*. Select each of these from the respective dropdown menus in order to recreate this example. The last specification is a filter, as the relevant subset only spanned ages from 18 to 30. We check the question *Do you need to filter data?*, add *age* as the *Filter Variable* and define the filter to be *x \> 17 & x \< 31* (by using logical and boolean operators from R). Going back to the sidebar on the left, we may now click *Run pipeline & Provide data to app*. After clicking this button, the analyses are run. The computations are finished as soon as the pop-up window (Calculation in progress. This may take a moment. Go to the *Data Selection* tab.) disappears. As soon as the data table appears in the *Data Selection* tab, it is ready for further use within the app. We can download a documentation of the analyses and data sets (from 0 Item Data to 5 Meta Pipe X) by clicking *Download MetaPipeX Output Directory*.

##### Visualizing Data from a Single Replication Project: Forest Plot

Now we select columns in the Data Selection tab and explore the data with the provided plot options. In the context of this example, our next step is to check if we were able to apply the data selection correctly. In order to do so, we create a forest plot, which we compare to Figure 1 in the publication by Dang et al. (2021). For the example, select *SMD* under *Site Statistics*, which by default additionally provides the according *SE*. After switching to the *Forest Plot* tab, we now click *Upload Data* and choose *Site Result SMD* as the *statistic of interest* and *Site Result SE SMD* as its *SE*. In order to add the labels of the units of aggregation (e.g., participating labs), select *Site*. The resulting forest plot should match that depicted in Dang et al. (2021).

## 4. Combining data sets in the Shiny App

In the final step of the tutorial we combine data sets from different projects and compare their kernel density estimations. To do so, we switch back to the *Upload Data* tab. The description of this step assumes that we have already provided the Dang et al. (2021) data to the app (see section 2.1). We only need to select *MetaPipeX* as the type of data, click *Browse* and select *MetaPipeX Data.csv* in the folder *lvl5_meta_pipe_x* within the *MetaPipeX Output* we previously created.

Press *Run pipeline & Provide data to app* and switch to the Data Selection tab. The data table should now contain data from both projects. The option to download the zip directory is not being displayed anymore. This option is only available for *IPD* uploads. By switching back to IPD in the *Upload Data* tab, the zip file download button is activated again and will provide the folder structure from the last *IPD* upload (of the current session).

##### Visualizing Data from Multiple Replication Projects: Kernel Density Estimation

In order to create the kernel density estimations, we need to check *Model Estimates* and *Tau* in *Meta- Analysis Results* and switch to the *Kernel Density Estimations* tab. Click *Upload Data*, choose *Site Result SMD* as the *site statistic of interest*, *MA Est SMD* as the *model estimate* and *MA Tau SMD* as *the according tau*. The kernel density estimation for the *RRR4* data (Sripada) shows study results rather evenly distributed around the null. While the mode for Dang et al. (2021) seems to be around the null as well, most replications, and thus the mean, depict a SMD \> 0. While the *RRR4* data exhibits a large amount of heterogeneity, the tau for Dang et al. (2021) is so small that it is not depicted in the graph. A possible explanation for this surprising result is that the project only spanned twelve replications and therefore has little power to detect heterogeneity.

##### Exporting Results

We can easily export any data set and plot created in the app. Below every plot display, a download button is provided that exports a pdf object. It is also possible to export it as a .png file by right-clicking the plot. Our current data selection (exclusions are applied) is exported from the *Data Selection* tab by clicking *Download Table Data*. The full *MetaPipeX* folder structure (including codebooks) is available through *Download MetaPipeX Output Directory* as a .zip file.

### References

-   Baumeister, Roy F ; Bratslavsky, Ellen ; Muraven, Mark ; Tice, Dianne M . Journal of Personality and Social Psychology Vol. 74, Iss. 5, (May 1998): 1252-1265. <DOI:10.1037/0022-3514.74.5.1252>

-   Bush, G., Shin, L., Holmes, J. et al. The Multi-Source Interference Task: validation study with fMRI in individual subjects. Mol Psychiatry 8, 60--70 (2003). <https://doi.org/10.1038/sj.mp.4001217>

-   Dang, J., Barker, P., Baumert, A., Bentvelzen, M., Berkman, E., Buchholz, N., Buczny, J., Chen, Z., De Cristofaro, V., de Vries, L., Dewitte, S., Giacomantonio, M., Gong, R., Homan, M., Imhoff, R., Ismail, I., Jia, L., Kubiak, T., Lange, F., ... Zinkernagel, A. (2021). A Multilab Replication of the Ego Depletion Effect. Social Psychological and Personality Science, 12(1), 14--24. <https://doi.org/10.1177/1948550619887702>

-   Hagger, M. S., Chatzisarantis, N. L. D., Alberts, H., Anggono, C. O., Batailler, C., Birt, A. R., Brand, R., Brandt, M. J., Brewer, G., Bruyneel, S., Calvillo, D. P., Campbell, W. K., Cannon, P. R., Carlucci, M., Carruth, N. P., Cheung, T., Crowell, A., De Ridder, D. T. D., Dewitte, S., ... Zwienenberg, M. (2016). A Multilab Preregistered Replication of the Ego-Depletion Effect. *Perspectives on Psychological Science*, *11*(4), 546 -573. <https://doi.org/10.1177/1745691616652873>
